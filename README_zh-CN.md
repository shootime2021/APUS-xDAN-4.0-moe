
<div align="center">
  <img src="https://github.com/shootime2021/APUS-xDAN-4.0-moe/assets/75604726/d32dd5e3-b901-49c7-aa35-067d6df91bce" width="500px"/>
  
  # APUS-xDAN-4.0(MoE)
  #  é¦–ä¸ªåƒäº¿MoEæ¶æ„å¤§æ¨¡å‹ æ•°å­¦ æ¨ç†èƒ½åŠ› è¶…è¶Šä¸‰åƒäº¿Grok-1ï¼
  #  å¯è½åœ°4090æ¶ˆè´¹æ˜¾å¡è¿è¡Œï¼Œæ­£å¼å¼€æºï¼ 

 

  <a href="#-performance">ğŸ“ŠPerformance </a> â€¢
  <a href="#-resources">âœ¨Resources </a> â€¢
  <a href="#-model-architecture">ğŸ“–Architecture </a> â€¢
  <a href="#-model-weights">ğŸ“‚Weights </a> â€¢
  <a href="#-install"> ğŸ”¨ Install </a> â€¢
  <a href="#-inference">ğŸš€Inference </a> â€¢
  <a href="#-acknowledgement">ğŸ¤ Acknowledgement </a>

  <br />
  <br />

  English | [ç®€ä½“ä¸­æ–‡](README_zh-CN.md)

</div>


> [!Important]
> <div align="center">
> <b>
> ğŸ“¢ æ¬¢è¿æ¥åˆ°å…¨çƒé¦–ä¸ªé«˜æ€§èƒ½åƒäº¿MoEå¼€æºå¤§æ¨¡å‹,æ¥è‡ªxDAN &APUSè”åˆè®­ç»ƒ! ğŸ“¢
> </b>
> <br>
> <b>
> ğŸ¤— è¶…å¼ºæ•°å­¦èƒ½åŠ› & æ¨ç†åˆ†æèƒ½åŠ› (GSM8k_Cot:79%) (MMLU:73.1%)</a>!
> </b>
> <br>
> <b>
> ğŸ™ è¯·éµå¾ªå¼€æºåè®®è‡ªç”±ä½¿ç”¨.
> </b>
> </div>


## News 
- ğŸ™Œ 04-03 æ¨¡å‹é“¾æ¥æ›´æ–°! [https://huggingface.co/xDAN-AI/APUS-xDAN-4.0-MOE]
- ğŸ™Œ 04-01 APUS-xDAN-4.0(MOE) æƒ…æ™¯ç­‰å¾…ï¼
- ğŸ™Œ 03-31 APUS-xDAN-4.0(MOE) é¦–ä¸ªä¸­è‹±æ–‡åƒäº¿MoEé«˜æ€§èƒ½æ¨¡å‹å¼€æº, å¯ä»¥è½åœ°åˆ°4090æ¶ˆè´¹çº§æ˜¾å¡è¿è¡Œã€‚

  


# ğŸ“Š Performance

## Comparison with Other Models

- Alignment By [xDAN-APUS4.0]([https://huggingface.co/xDAN-AI/APUS-xDAN-4.0-MOE))

> Performances generated from different evaluation toolkits are different due to the prompts, settings and implementation details.



| BenchMark        | Mode | APUS-xDAN-4.0(MoE) | Mixtral-8x7B(MoE) |  Llama2-70B | Grok-1ï¼ˆMoEï¼‰ |
|-----------------|------|-----------------|--------------|-------------|-------------------|
| Total Params æ€»å‚æ•°  |  GEN   |      136B         |     12B      |     70B     |       314B         |
| Active Params æ¿€æ´»å‚æ•°  |  GEN   |      60B         |     12B      |     70B     |       78.5B         |
| MMLU æ¨ç†åˆ†æ           | PPL  | **73.1**            | 71.3         | 69.7        | 73.0             |
| BIG-Bench-Hard å¤šå­¦ç§‘ä»»åŠ¡  | GEN  | 66.4            | 67.1         | 64.9        | 71.7              | 
| GSM-8K K12æ•°å­¦         | GEN  | **79.2**         | 65.7         | 63.4        | 62.9              |
| MATH   æ•°å­¦         | GEN  | **29.5**         | 22.7         | 12.0        | 23.9              | 

# âœ¨ Resources
| Model       | Quantized | Size   | Context     | Hardware Requirement     |
|-------------|-----------|--------|--------------------------| --------------------------|
| APUS-xDAN4.0-MoE-0402.Q2_K.gguf       | Q2_K        | 39G  | 32k | 2x24G GPU memory |
| APUS-xDAN4.0-MoE-0402.IQ3_XXS.gguf      | IQ3_XXS        | 41G | 32k | 2x24G GPU memory |
| APUS-xDAN4.0-MoE-0402.Q3_K_M_Matrix.gguf      | Q3_K_M        | 51G | 32k | 2x24G GPU memory |
| APUS-xDAN4.0-MoE-0402.Q4_K_M.gguf | Q4_K_M       | 64G  | 32k | 3x24G GPU memory |

## Deployment
- [x] [Inference with llama.cpp](https://github.com/ggerganov/llama.cpp)
- [x] [Inference with vLLM](https://github.com/vllm-project/vllm)

# ğŸ“– Model Architecture

>  The APUS-xDAN-4.0(MoE) model is mainly composed of 32 identical MoEtransformer blocks. The main difference between the MoEtransformer block and the ordinary transformer block is that the FFN layer is replaced by the **MoE FFN** layer. In the MoE FFN layer, the tensor first goes through a gate layer to calculate the scores of each expert, and then selects the top-k experts from the 8 experts based on the expert scores. The tensor is aggregated through the outputs of the top-k experts, thereby obtaining the final output of the MoE FFN layer. Each expert consists of 3 linear layers. It is worth noting that all Norm Layers of APUS-xDAN4.0 MoE also use RMSNorm, which is the same as LLama. In the attention layer, the QKV matrix in the APUS-xDAN4.0 MoE has a Q matrix shape of (4096,4096) and K and V matrix shapes of (4096,1024).

We plot the architecture as the following:

<div align="center">
  <img src="https://github.com/shootime2021/APUS-xDAN-4.0-moe/assets/75604726/5c86b15a-5858-48bd-a6b3-62d1c326b6f4" width="800px"/>
</div>

# ğŸ“‚ Model Weights

## Hugging Face Format

- [Downlowd Model](https://huggingface.co/xDAN-AI/APUS-xDAN-4.0-MOE)

## Raw Format

You can download the checkpoints by magnet or Hugging Face

### Download via HF

- [APUS-xDAN-4.0-MOE](https://huggingface.co/xDAN-AI/APUS-xDAN-4.0-MOE)

> If you are unable to access Hugging Face, please try [hf-mirror](https://huggingface.co/xDAN-AI/APUS-xDAN-4.0-MOE)


```bash
# Download the Hugging Face
git lfs install
git clone https://huggingface.co/xDAN-AI/APUS-xDAN-4.0-MOE
# Merge Files(Only for HF)
cd APUS-xDAN-4.0-MOE/



# ğŸš€ Inference

## Text Completion 
```bash
./main -m APUS-xDAN4.0-MoE-0402.Q2_K.gguf   --n-gpu-layers 99 \
--prompt "You are a helpful assistant named APUS-xDAN4.0 MoE. " --chatml \
--interactive \
--temp 0.7 \
--ctx-size 4096
```


# ğŸ–Šï¸ Citation


```latex
@misc{2023opencompass,
    title={xDAN-APUS4.0: High Performance Alignment Model Trainer.},
    author={xDAN-APUS4.0 Contributors},
    howpublished = {\url{https://github.com/shootime2021/APUS-xDAN-4.0-moe}},
    year={2024}
}
```
